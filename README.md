# StreamDeploy LLM App Scaffold

Run the following from the root of the llm-app-scaffold directory:

```
docker-compose up --build
```

To use a model from Ollama, run Ollama pull. For example, to use mistral, run '''ollama pull mistral''' then run the docker compose command